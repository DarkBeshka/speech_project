# Отчет по проекту русскоязычного синтеза речи на базе GlowTTS

## Введение
На работе имею задачу разработки видеокурса, в рамках которой нужно озвучить текст и записать скринкасты. Поэтому хотела попробовать реализовать модель TTS, которая смогла бы озвучивать текст вместо меня.

## Используемые инструменты и архитектура

### Данные и аудиоподготовка
Корпус: [Ruslan](https://ruslan-corpus.github.io/)

Аудио предварительно ресэмплируется до 22 050 Гц скриптом `resample_wavs.py`(исходник в 40кГц), а правильность частоты проверяется `check_sample_rate.py`.

Метаданные: `metadata_train.txt` и `metadata_val.txt` (ruslan formatter) для загрузки пар текст–аудио.

Обработка аудио: `AudioProcessor` генерирует 80-мерные мел-спектрограммы (`fft_size=1024`, `hop_length=256`, `win_length=1024`, `spec_gain=20`, нормализация `signal_norm=True`), взяла за основу стандартную фронтенд-конфигурацию для вокодера Griffin–Lim(восстановление фазы по известной амплитуде STFT).

### Текстовое представление и словарь

Символьный (grapheme) токенизатор с кастомным `CharactersConfig`, в котором прописан полный набор строчных и прописных кириллических символов, а также пунктуацию - подобраны эмпирически под датасет.

Phonemizer отключен (`use_phonemes=False`), что упрощает пайплайн, но делает модель чувствительной к орфографии и пунктуации.

### Архитектура модели GlowTTS
Базовая модель: **GlowTTS** (flow-based text-to-mel), параметрическая TTS-модель. Преобразует латентное распределение в распределение мел-спектрограмм через обратимое отображение. Взяла как простой и быстрый вариант, так как нет мощной видеокарты(NVIDIA 4060 обычная).

- Энкодер: Transformer (`encoder_type="rel_pos_transformer"`), переводящий последовательность символов в скрытые представления.
- Duration predictor: моделирует длительности символов, что обеспечивает монотонное выравнивание без внимания. Loss для длительностей добавляется к отрицательному лог-правдоподобию.
- Декодер/flow: 12 flow-блоков с kernel size 5 и 4 слоями в блоке, скрытые размеры 192–256. Используется `mean_only=True` для стабильности.
- Выход: мел-спектрограммы (80 каналов) далее инвертируются Griffin–Lim при синтезе через `Synthesizer` (см. `inference.py`).

### Обучение и оптимизация

Оптимизатор: RAdam с `lr=2e-4`, `betas=(0.9, 0.998)`, `weight_decay=1e-6`.

Планировщик: NoamLR с увеличенным `warmup_steps=8000`, для медленного рост шага и сглаживания начальной фазы обучения(попытка исправить проблему взрывающихся градиентов). Также для этой цели применены настройки: `grad_clip=1.0`, mixed precision (`fp16`) и малый `batch_size=8`. Дополнительно включена защита от блокировок логов на Windows при очистке эксперимента.

Реализовано сохранение чекпоинтов каждые 10 000 шагов.

### Мониторинг и диагностика

TensorBoard запускается через `view_tensorboard.py`.

### Инференс
Пример инференса (`inference.py`): `Synthesizer` загружает конфиг и чекпоинт GlowTTS, выполняет tts() для кириллического текста и сохраняет аудио. В качестве вокодера применяется встроенный Griffin–Lim.

### Возможные альтернативы и сравнительный анализ
- Tacotron 2 + нейровокодер (WaveGlow/HiFi-GAN): автогрессия дает высокое качество и гибкость, но медленный инференс и чувствительность к выравниванию; требует attention-масок и более строгого контроля данных.
- FastSpeech 2 + HiFi-GAN: полностью неавтогрессивный, быстрый инференс; опирается на teacher модель для длительностей или на внешний aligner; меньше проблема с взрывами градиентов, но чуть ниже натуральность без тщательного тюнинга.
- VITS: end-to-end потоковая генерация waveforms с VAE+GAN; дает state-of-the-art качество, но сложнее в настройке, требует больше данных и аккуратной стабилизации GAN-компоненты.
- Grad-TTS / FlowTTS: диффузионные или потоковые аналоги с монотонным выравниванием; дают плавное управление длительностью и устойчивость к шуму в данных, но инференс тяжелее(несколько шагов диффузии) и обучать их дороже.

## Анализ хода разработки
Первые две недели были посвящены решению проблем с зависимостями, блокировкой процессов Windows(по окончании эпохи обучение прерывалось так как не могло получить доступ к папке с exp модели для перезаписи), а также недостатком памяти видеокарты.

Следующие полторы недели были посвящены обучению модели и тестированию на разном количестве эпох. В репозитории представлены результаты, каждый файл - синтезированная запись фразы _"Привет. Это тест модели"_:
- файл `first_output.py` - модель обучена на 10 эпохах;
- `second_output.py` - на 20 эпохах;
- `third_output.py` - на 32 эпохах;
- `fourth_output.py` - на 50 эпохах;

50 эпох модель обучалась более 8 часов непрерывной работы(около 6 эпох за час).

После этого был реализован файл `view_tensorboard.py`, с помощью которого была выявлена проблема взраывающихся градиентов - после определенного шага нормы градиентов на тесте резко подскакивали(вплоть до значений 600-1000), а на графике трейна была "пила". В соответствии с этими результатами были предприняты попытки исправить ситуацию следующими мерами:
- Уменьшен Learning Rate(уменьшен в 5 раз) - градиенты не так сильно влияют на обновление весов, что позволяет модели обучаться более плавно;
- Более агрессивный Gradient Clipping(уменьшен в 5 раз) - ограничивает максимальную величину градиентов;
- Увеличен Warmup Steps(в 2 раза) - дает модели время "разогреться" перед полным обучением;

Остальные метрики(alignment, loss, пр.) были в пределах нормы и демонстрировали ожидаемое поведение.

На этом этапе также была реализована обретка для Telegram-бота, который принимает от пользователя сообщение - текст, который необходимо озвучить, а выдает аудиосообщение с озвучкой этой фразы.

Далее модель заново была обучена на 32 эпохах, однако результат оказался еще хуже, чем был до этого, что, к тому же, не решило проблему с градиентами. Скриншоты из TensorBoard прилагаю:
<img width="1766" height="717" alt="image" src="https://github.com/user-attachments/assets/d068b2d3-d309-4106-826f-635a955d270e" />
<img width="1969" height="922" alt="image" src="https://github.com/user-attachments/assets/8da2916a-b56b-469d-a36e-1f9853d2156f" />
<img width="1972" height="388" alt="image" src="https://github.com/user-attachments/assets/472a4271-a446-4507-81ed-001f842f89c9" />
<img width="1974" height="920" alt="image" src="https://github.com/user-attachments/assets/26cd4a5d-fac8-4e1f-9ad4-f265c81f22e6" />

Видеорезультат работы Telegram-бота на видео(звук в аудио есть, просто тихий и не похож на речь - модель после принятия мер к градиентам отрабатывает хуже):

https://github.com/user-attachments/assets/120d63e7-a9ff-457b-987a-81adf24f5380

Всего работа над проектом заняла чуть более месяца.

## Результаты и выводы

Полученный результат приводит к следующим выводам:
1. Нужно попробовать найти другие способы держать под контролем градиенты либо обучить другую модель, либо использовать другие данные, однако с учетом моего железа для этого потребуется огромное количество времени;
2. Количество времени и сил, которое мне понадобилось на разработку модели, которая привела к получению даже далекого от удовлетворительного результата, несопоставимо велико. Это можно объяснить тем, что я никогда раньше не обучала свои собственные модели;
