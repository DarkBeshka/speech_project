# –ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –≤–∑—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤ (Gradient Explosion)

## –ü—Ä–æ–±–ª–µ–º–∞

–í—ã –Ω–∞–±–ª—é–¥–∞–µ—Ç–µ —Å–ª–µ–¥—É—é—â–∏–µ —Å–∏–º–ø—Ç–æ–º—ã:
- **–ù–∞ —Ç–µ—Å—Ç–µ**: `grad_norm` –¥–æ—Å—Ç–∏–≥–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–π >600
- **–ì—Ä–∞—Ñ–∏–∫ –ø–æ—Ö–æ–∂ –Ω–∞ —Å–∏–≥–º–æ–∏–¥—É**: –ø–ª–∞–≤–Ω—ã–π —Ä–æ—Å—Ç –¥–æ 100, —Ä–µ–∑–∫–∏–π —Å–∫–∞—á–æ–∫ –¥–æ 500, –∑–∞—Ç–µ–º –ø–ª–∞–≤–Ω—ã–π —Ä–æ—Å—Ç –¥–æ 600
- **–ù–∞ –æ–±—É—á–µ–Ω–∏–∏**: `grad_norm` –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —Ä–∞—Å—Ç–µ—Ç –¥–æ 1000
- **–ü–æ—Å—Ç–æ—è–Ω–Ω—ã–µ —Å–∫–∞—á–∫–∏** —Å —Ä–∞–∑–Ω—ã–º –¥–∏–∞–ø–∞–∑–æ–Ω–æ–º –≤–ø–ª–æ—Ç—å –¥–æ 600 –µ–¥–∏–Ω–∏—Ü

–≠—Ç–æ –∫–ª–∞—Å—Å–∏—á–µ—Å–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ **–≤–∑—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤** (gradient explosion).

---

## –ü—Ä–∏—á–∏–Ω—ã –≤–∑—Ä—ã–≤–∞—é—â–∏—Ö—Å—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤

1. **–°–ª–∏—à–∫–æ–º –≤—ã—Å–æ–∫–∏–π learning rate**
   - –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã —É–º–Ω–æ–∂–∞—é—Ç—Å—è –Ω–∞ –±–æ–ª—å—à–æ–π –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç
   - –ù–µ–±–æ–ª—å—à–∏–µ –æ—à–∏–±–∫–∏ –±—ã—Å—Ç—Ä–æ –Ω–∞–∫–∞–ø–ª–∏–≤–∞—é—Ç—Å—è

2. **–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–π gradient clipping**
   - –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ –æ–≥—Ä–∞–Ω–∏—á–∏–≤–∞—é—Ç—Å—è –ø–æ –≤–µ–ª–∏—á–∏–Ω–µ
   - –ü–æ–∑–≤–æ–ª—è–µ—Ç –∏–º —Ä–∞—Å—Ç–∏ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω–æ

3. **–ù–µ—Å—Ç–∞–±–∏–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞**
   - –ì–ª—É–±–æ–∫–∏–µ —Å–µ—Ç–∏ —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Å–ª–æ–µ–≤
   - –ü—Ä–æ–±–ª–µ–º—ã —Å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–µ–π –≤–µ—Å–æ–≤

4. **–ü—Ä–æ–±–ª–µ–º—ã —Å –¥–∞–Ω–Ω—ã–º–∏**
   - –û—á–µ–Ω—å –¥–ª–∏–Ω–Ω—ã–µ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
   - –≠–∫—Å—Ç—Ä–µ–º–∞–ª—å–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è –≤ –¥–∞–Ω–Ω—ã—Ö

---

## –†–µ—à–µ–Ω–∏—è (—É–∂–µ –ø—Ä–∏–º–µ–Ω–µ–Ω—ã –≤ train_ruslan_glowtts.py)

### 1. –£–º–µ–Ω—å—à–µ–Ω Learning Rate
```python
lr=0.0002,  # –ë—ã–ª–æ 0.001 –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é (—É–º–µ–Ω—å—à–µ–Ω–æ –≤ 5 —Ä–∞–∑)
```

**–ü–æ—á–µ–º—É —ç—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç:**
- –ú–µ–Ω—å—à–∏–π learning rate = –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ
- –ì—Ä–∞–¥–∏–µ–Ω—Ç—ã –Ω–µ —Ç–∞–∫ —Å–∏–ª—å–Ω–æ –≤–ª–∏—è—é—Ç –Ω–∞ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –≤–µ—Å–æ–≤
- –ü–æ–∑–≤–æ–ª—è–µ—Ç –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—Ç—å—Å—è –±–æ–ª–µ–µ –ø–ª–∞–≤–Ω–æ

### 2. –ë–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π Gradient Clipping
```python
grad_clip=1.0,  # –ë—ã–ª–æ 5.0 (—É–º–µ–Ω—å—à–µ–Ω–æ –≤ 5 —Ä–∞–∑)
```

**–ü–æ—á–µ–º—É —ç—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç:**
- –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ—Ç –º–∞–∫—Å–∏–º–∞–ª—å–Ω—É—é –≤–µ–ª–∏—á–∏–Ω—É –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
- –ü—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –≤–∑—Ä—ã–≤ –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
- –°—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—É—á–µ–Ω–∏–µ

### 3. –£–≤–µ–ª–∏—á–µ–Ω Warmup Steps
```python
lr_scheduler_params={
    "warmup_steps": 8000,  # –ë—ã–ª–æ 4000 (—É–≤–µ–ª–∏—á–µ–Ω–æ –≤ 2 —Ä–∞–∑–∞)
}
```

**–ü–æ—á–µ–º—É —ç—Ç–æ –ø–æ–º–æ–≥–∞–µ—Ç:**
- –ü–ª–∞–≤–Ω—ã–π —Å—Ç–∞—Ä—Ç –æ–±—É—á–µ–Ω–∏—è
- Learning rate –ø–æ—Å—Ç–µ–ø–µ–Ω–Ω–æ —É–≤–µ–ª–∏—á–∏–≤–∞–µ—Ç—Å—è –æ—Ç 0 –¥–æ —Ü–µ–ª–µ–≤–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è
- –î–∞–µ—Ç –º–æ–¥–µ–ª–∏ –≤—Ä–µ–º—è "—Ä–∞–∑–æ–≥—Ä–µ—Ç—å—Å—è" –ø–µ—Ä–µ–¥ –ø–æ–ª–Ω—ã–º –æ–±—É—á–µ–Ω–∏–µ–º

---

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏

### –ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è:

#### 1. –ï—â–µ –±–æ–ª—å—à–µ —É–º–µ–Ω—å—à–∏—Ç–µ learning rate
```python
lr=0.0001,  # –ï—â–µ –≤ 2 —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ
```

#### 2. –ï—â–µ –±–æ–ª–µ–µ –∞–≥—Ä–µ—Å—Å–∏–≤–Ω—ã–π gradient clipping
```python
grad_clip=0.5,  # –ï—â–µ –≤ 2 —Ä–∞–∑–∞ –º–µ–Ω—å—à–µ
```

#### 3. –£–º–µ–Ω—å—à–∏—Ç–µ batch size
```python
batch_size=4,  # –ë—ã–ª–æ 8
```
**–ü–æ—á–µ–º—É:** –ú–µ–Ω—å—à–∏–π batch size = –±–æ–ª–µ–µ —Å—Ç–∞–±–∏–ª—å–Ω—ã–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã

#### 4. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –¥–ª–∏–Ω—É –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∞—É–¥–∏–æ —Ñ–∞–π–ª—ã –Ω–µ —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–µ
- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ `max_text_len` –∏ `max_audio_len` –≤ –∫–æ–Ω—Ñ–∏–≥–µ

#### 5. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ gradient accumulation
–ï—Å–ª–∏ —É–º–µ–Ω—å—à–µ–Ω–∏–µ batch size –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –∏–∑-–∑–∞ –ø–∞–º—è—Ç–∏:
```python
# –í TrainerArgs –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å gradient accumulation
# –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –º–µ–Ω—å—à–∏–π —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π batch size
```

#### 6. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—é –≤–µ—Å–æ–≤
- –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤–µ—Å–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É—é—Ç—Å—è –ø—Ä–∞–≤–∏–ª—å–Ω–æ
- GlowTTS –¥–æ–ª–∂–µ–Ω –¥–µ–ª–∞—Ç—å —ç—Ç–æ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏, –Ω–æ –º–æ–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å

---

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ—Å–ª–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π

–ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π —Å–ª–µ–¥–∏—Ç–µ –∑–∞:

1. **grad_norm –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å:**
   - –í –¥–∏–∞–ø–∞–∑–æ–Ω–µ **1-50** (–∏–¥–µ–∞–ª—å–Ω–æ)
   - –ù–µ –¥–æ–ª–∂–µ–Ω –ø—Ä–µ–≤—ã—à–∞—Ç—å **100**
   - –ù–µ –¥–æ–ª–∂–µ–Ω –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —Ä–∞—Å—Ç–∏

2. **Loss –¥–æ–ª–∂–µ–Ω:**
   - –°—Ç–∞–±–∏–ª—å–Ω–æ —Å–Ω–∏–∂–∞—Ç—å—Å—è
   - –ù–µ –∏–º–µ—Ç—å —Ä–µ–∑–∫–∏—Ö —Å–∫–∞—á–∫–æ–≤
   - –ù–µ –∑–∞—Å—Ç—Ä–µ–≤–∞—Ç—å –Ω–∞ –≤—ã—Å–æ–∫–∏—Ö –∑–Ω–∞—á–µ–Ω–∏—è—Ö

3. **–ì—Ä–∞—Ñ–∏–∫ grad_norm –¥–æ–ª–∂–µ–Ω:**
   - –ë—ã—Ç—å –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω–æ —Å—Ç–∞–±–∏–ª—å–Ω—ã–º
   - –ù–µ –∏–º–µ—Ç—å —Ä–µ–∑–∫–∏—Ö —Å–∫–∞—á–∫–æ–≤
   - –ù–µ —Ä–∞—Å—Ç–∏ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ

---

## –ü–æ—à–∞–≥–æ–≤—ã–π –ø–ª–∞–Ω –¥–µ–π—Å—Ç–≤–∏–π

1. ‚úÖ **–ü—Ä–∏–º–µ–Ω–µ–Ω—ã –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è** –≤ `train_ruslan_glowtts.py`:
   - –£–º–µ–Ω—å—à–µ–Ω learning rate –¥–æ 0.0002
   - –£–º–µ–Ω—å—à–µ–Ω grad_clip –¥–æ 1.0
   - –£–≤–µ–ª–∏—á–µ–Ω warmup_steps –¥–æ 8000

2. üîÑ **–ó–∞–ø—É—Å—Ç–∏—Ç–µ –Ω–æ–≤–æ–µ –æ–±—É—á–µ–Ω–∏–µ** —Å —ç—Ç–∏–º–∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∞–º–∏

3. üìä **–ú–æ–Ω–∏—Ç–æ—Ä—å—Ç–µ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã** –≤ TensorBoard:
   ```bash
   python view_tensorboard.py
   ```

4. ‚ö†Ô∏è **–ï—Å–ª–∏ –ø—Ä–æ–±–ª–µ–º–∞ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç—Å—è:**
   - –ï—â–µ —É–º–µ–Ω—å—à–∏—Ç–µ learning rate –¥–æ 0.0001
   - –ï—â–µ —É–º–µ–Ω—å—à–∏—Ç–µ grad_clip –¥–æ 0.5
   - –£–º–µ–Ω—å—à–∏—Ç–µ batch_size –¥–æ 4

5. ‚úÖ **–ö–æ–≥–¥–∞ –≥—Ä–∞–¥–∏–µ–Ω—Ç—ã —Å—Ç–∞–±–∏–ª–∏–∑–∏—Ä—É—é—Ç—Å—è:**
   - –ü—Ä–æ–¥–æ–ª–∂–∞–π—Ç–µ –æ–±—É—á–µ–Ω–∏–µ
   - –°–ª–µ–¥–∏—Ç–µ –∑–∞ loss - –æ–Ω –¥–æ–ª–∂–µ–Ω —Å–Ω–∏–∂–∞—Ç—å—Å—è
   - –ü—Ä–æ–≤–µ—Ä—è–π—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ç–µ–∑–∞

---

## –û–∂–∏–¥–∞–µ–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã

–ü–æ—Å–ª–µ –ø—Ä–∏–º–µ–Ω–µ–Ω–∏—è –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–π –≤—ã –¥–æ–ª–∂–Ω—ã —É–≤–∏–¥–µ—Ç—å:

- ‚úÖ `grad_norm` –≤ –¥–∏–∞–ø–∞–∑–æ–Ω–µ 1-50
- ‚úÖ –°—Ç–∞–±–∏–ª—å–Ω—ã–π –≥—Ä–∞—Ñ–∏–∫ –±–µ–∑ —Ä–µ–∑–∫–∏—Ö —Å–∫–∞—á–∫–æ–≤
- ‚úÖ Loss –ø–ª–∞–≤–Ω–æ —Å–Ω–∏–∂–∞–µ—Ç—Å—è
- ‚úÖ –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è —Å—Ç–∞–±–∏–ª—å–Ω–æ
- ‚úÖ –ö–∞—á–µ—Å—Ç–≤–æ —Å–∏–Ω—Ç–µ–∑–∞ —É–ª—É—á—à–∞–µ—Ç—Å—è

---

## –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–µ —Ä–µ—Å—É—Ä—Å—ã

- [Understanding Gradient Explosion](https://machinelearningmastery.com/exploding-gradients-in-neural-networks/)
- [Gradient Clipping in PyTorch](https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html)
- [Learning Rate Scheduling](https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate)

